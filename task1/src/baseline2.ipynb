{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad2ee83-1043-4471-97c2-870d2f76c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from transformers import BertModel, BertTokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "ROOT = Path(\"../data/\")\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "LABELS = [\n",
    "    \"Appeal_to_Values\",\n",
    "    \"Loaded_Language\",\n",
    "    \"Consequential_Oversimplification\",\n",
    "    \"Causal_Oversimplification\",\n",
    "    \"Questioning_the_Reputation\",\n",
    "    \"Straw_Man\",\n",
    "    \"Repetition\",\n",
    "    \"Guilt_by_Association\",\n",
    "    \"Appeal_to_Hypocrisy\",\n",
    "    \"Conversation_Killer\",\n",
    "    \"False_Dilemma-No_Choice\",\n",
    "    \"Whataboutism\",\n",
    "    \"Slogans\",\n",
    "    \"Obfuscation-Vagueness-Confusion\",\n",
    "    \"Name_Calling-Labeling\",\n",
    "    \"Flag_Waving\",\n",
    "    \"Doubt\",\n",
    "    \"Appeal_to_Fear-Prejudice\",\n",
    "    \"Exaggeration-Minimisation\",\n",
    "    \"Red_Herring\",\n",
    "    \"Appeal_to_Popularity\",\n",
    "    \"Appeal_to_Authority\",\n",
    "    \"Appeal_to_Time\",\n",
    "]\n",
    "\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def parse_text(text, span_objs, tokenizer: PreTrainedTokenizerFast, labels):\n",
    "    assert isinstance(\n",
    "        tokenizer, PreTrainedTokenizerFast\n",
    "    ), \"Must be a sub-class of PreTrainedTokenizerFast\"\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    label_encoding = [\n",
    "        [\n",
    "            0,\n",
    "        ]\n",
    "        * len(encoding.tokens())\n",
    "        for _ in labels\n",
    "    ]\n",
    "\n",
    "    for span in span_objs:\n",
    "        label_start, label_end, label = span[\"start\"], span[\"end\"], span[\"technique\"]\n",
    "        # l is for sequence number\n",
    "        for l, (token_start, token_end) in enumerate(encoding.offset_mapping):\n",
    "            if is_inside(token_start, token_end, label_start, label_end):\n",
    "                label_encoding[labels.index(label)][l] = 1\n",
    "\n",
    "    return encoding, label_encoding\n",
    "\n",
    "\n",
    "def is_inside(token_start, token_end, label_start, label_end):\n",
    "    return token_start >= label_start and token_end <= label_end\n",
    "\n",
    "\n",
    "def find_consecutive_trues(flags):\n",
    "    \"\"\"\n",
    "    This function takes an array of boolean flags and returns a list of ranges\n",
    "    of all consecutive true values.\n",
    "\n",
    "    Args:\n",
    "        flags: A list of boolean flags.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple represents a range of consecutive\n",
    "        true values. The tuple contains the starting and ending indices (inclusive)\n",
    "        of the range.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    start_idx = None\n",
    "    for i, flag in enumerate(flags):\n",
    "        if flag and start_idx is None:\n",
    "            start_idx = i\n",
    "        elif not flag and start_idx is not None:\n",
    "            ranges.append((start_idx, i - 1))\n",
    "            start_idx = None\n",
    "    if start_idx is not None:\n",
    "        ranges.append((start_idx, len(flags) - 1))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# flags = [1, 1, 0, 0, 1, 1, 1]\n",
    "# ranges = find_consecutive_trues(flags)\n",
    "# print(ranges)  # Output: [(0, 1), (3, 5)]\n",
    "\n",
    "\n",
    "\n",
    "def find_consecutive_trues(flags):\n",
    "    \"\"\"\n",
    "    This function takes an array of boolean flags and returns a list of ranges\n",
    "    of all consecutive true values.\n",
    "\n",
    "    Args:\n",
    "        flags: A list of boolean flags.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple represents a range of consecutive\n",
    "        true values. The tuple contains the starting and ending indices (inclusive)\n",
    "        of the range.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    start_idx = None\n",
    "    for i, flag in enumerate(flags):\n",
    "        if flag and start_idx is None:\n",
    "            start_idx = i\n",
    "        elif not flag and start_idx is not None:\n",
    "            ranges.append((start_idx, i - 1))\n",
    "            start_idx = None\n",
    "    if start_idx is not None:\n",
    "        ranges.append((start_idx, len(flags) - 1))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def parse_sample(sample):\n",
    "    encoding, label_encoding = parse_text(\n",
    "        sample[\"text\"], sample[\"labels\"], tokenizer, LABELS\n",
    "    )\n",
    "    \n",
    "    return {\"encoding\": encoding, 'id': sample['id'], **encoding, \"labels\": label_encoding}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580853f1-c8ca-44d0-b918-775ecbd0ec08",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "847153a2-5067-4e84-9797-967e5cc6509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from transformers import BertModel, BertTokenizerFast, PreTrainedTokenizerFast\n",
    "\n",
    "label2id = {label: id for id, label in enumerate(LABELS)}\n",
    "id2label = {id: label for id, label in enumerate(LABELS)}\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "\n",
    "class CustomBertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        config.output_hidden_states = True\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout\n",
    "            if config.classifier_dropout is not None\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs['hidden_states'][1:][7]\n",
    "\n",
    "\n",
    "        # sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(reduction='none')\n",
    "            loss = loss_fct(logits, labels.transpose(1, 2).float())\n",
    "            loss = (loss * attention_mask.unsqueeze(dim=2)).mean()\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac43f9fc-cbc5-444e-86ca-4d7772071d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fd10bd-8c76-4bcd-ab0f-a0309e028ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on /home/riyadh/codes/nlp/araieval_arabicnlp24/task1/src/../data/task1_train.jsonl\n",
      "Validating on /home/riyadh/codes/nlp/araieval_arabicnlp24/task1/src/../data/task1_dev.jsonl\n",
      "\n",
      "{'id': '7365', 'text': 'ÿ™ÿ≠ÿ∞Ÿäÿ±ÿßÿ™ ŸÖŸÜ ÿ≠ÿ±ÿ® ÿ¨ÿØŸäÿØÿ© ŸÅŸä ÿ≠ÿßŸÑ ŸÅÿ¥ŸÑ ÿßŸÑÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™ ÿßŸÑŸÇÿßÿØŸÖÿ©', 'labels': [{'start': 0, 'end': 50, 'technique': 'Appeal_to_Fear-Prejudice', 'text': 'ÿ™ÿ≠ÿ∞Ÿäÿ±ÿßÿ™ ŸÖŸÜ ÿ≠ÿ±ÿ® ÿ¨ÿØŸäÿØÿ© ŸÅŸä ÿ≠ÿßŸÑ ŸÅÿ¥ŸÑ ÿßŸÑÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™ ÿßŸÑŸÇÿßÿØŸÖÿ©'}, {'start': 11, 'end': 14, 'technique': 'Loaded_Language', 'text': 'ÿ≠ÿ±ÿ®'}], 'type': 'tweet'}\n",
      "dict_keys(['encoding', 'id', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(ROOT)\n",
    "TRAIN_FILE = ROOT / \"task1_train.jsonl\"\n",
    "DEV_FILE = ROOT / \"task1_dev.jsonl\"\n",
    "\n",
    "print(f\"Training on {TRAIN_FILE.absolute()}\\nValidating on {DEV_FILE.absolute()}\\n\")\n",
    "\n",
    "with jsonlines.open(TRAIN_FILE) as jsonfile:\n",
    "    for obj in jsonfile:\n",
    "        print(obj)\n",
    "\n",
    "        parsed = parse_sample(obj)\n",
    "        print(parsed.keys())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3a1191-d94b-4398-80c5-3797a8a7db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class DatasetFromJson(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          data_path (str): Path to the JSONLines file containing map style data.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.encodings = []\n",
    "        self.tensors = []\n",
    "        self.raw = []\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads map style data from the JSONLines file.\n",
    "        \"\"\"\n",
    "        with open(self.data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                if 'labels' not in data:\n",
    "                    data['labels'] = []\n",
    "                    \n",
    "                sample = parse_sample(data)\n",
    "                self.encodings.append(sample[\"encoding\"])\n",
    "\n",
    "                del sample[\"encoding\"], sample[\"id\"]\n",
    "                \n",
    "                self.tensors.append(sample)\n",
    "                self.raw.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "          idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "          dict: A dictionary containing the map style features.\n",
    "        \"\"\"\n",
    "        return self.tensors[idx], self.encodings[idx], self.raw[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8608cd34-9cbf-4507-90a0-1ef598a0b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DatasetFromJson(TRAIN_FILE)\n",
    "val_ds = DatasetFromJson(DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e86d57-e7e4-4564-a142-44d539f72eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self, return_raw=False):\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "        self.return_raw = return_raw\n",
    "        \n",
    "    def __call__(self, data: List):\n",
    "        # 'encoding', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'\n",
    "        encodings = []\n",
    "        raws = []\n",
    "        tensors = []\n",
    "        \n",
    "        for tensor, encoding, raw in data:\n",
    "            encodings.append(encoding)\n",
    "            raws.append(raw)\n",
    "            tensors.append(tensor)\n",
    "\n",
    "        batch = self.data_collator(tensors)\n",
    "        \n",
    "        return {\n",
    "            'tensors': batch,\n",
    "            'encodings': encodings,\n",
    "            'raws': raws\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b386c2-a0d6-4cd5-8250-e28318e5811a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collate_fn = CollateFn()\n",
    "train_dl = DataLoader(train_ds, batch_size=3, collate_fn = collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=3, collate_fn=CollateFn(return_raw=True))\n",
    "\n",
    "batch = next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a4ea4c-181a-48cf-9fc0-4387bd47e32e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'task1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../task1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m task1 \u001b[38;5;28;01mas\u001b[39;00m t1scorer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n",
      "File \u001b[0;32m~/codes/nlp/araieval_arabicnlp24/task1/src/../../task1/scorer/task1.py:12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtask1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_checker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtask1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_format\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# from format_checker.task1 import check_format\u001b[39;00m\n\u001b[1;32m     15\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'task1'"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../task1\")\n",
    "\n",
    "from scorer import task1 as t1scorer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3fa8512-b3c1-4059-86c2-791b3bf880c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_output(objs):\n",
    "    labels_per_par = defaultdict(list)\n",
    "    for obj in objs:\n",
    "        par_id = obj[\"id\"]\n",
    "        labels = obj[\"labels\"]\n",
    "        labels_per_par[par_id] = t1scorer.process_labels(labels)\n",
    "    return labels_per_par\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a96884-bdca-48a3-8fc9-f207a86a916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label_encoding(text, encoding, label_encoding, labels):\n",
    "    label_encoding = label_encoding > 0\n",
    "    span_objs = []\n",
    "    word_ids = encoding.word_ids()\n",
    "    for i, label in enumerate(labels):\n",
    "        flags = label_encoding[i]\n",
    "        span_ranges = find_consecutive_trues(flags)\n",
    "        for start_idx, end_idx in span_ranges:\n",
    "            start_word_id, end_word_id = word_ids[start_idx], word_ids[end_idx]\n",
    "            \n",
    "            if start_word_id is None or end_word_id is None:\n",
    "                # skip padding\n",
    "                continue\n",
    "                \n",
    "            (start_char_idx, _), (_, end_char_idx) = encoding.word_to_chars(\n",
    "                start_word_id\n",
    "            ), encoding.word_to_chars(end_word_id)\n",
    "\n",
    "            span_text=\"\"\n",
    "            if text is not None:\n",
    "                span_text = text[start_char_idx: end_char_idx]\n",
    "\n",
    "            obj = {\n",
    "                \"technique\": label,\n",
    "                \"start\": start_char_idx,\n",
    "                \"end\": end_char_idx,\n",
    "                \"text\": span_text,\n",
    "            }\n",
    "\n",
    "            span_objs.append(obj)\n",
    "    return span_objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3742b667-31db-451f-a2b5-ab60ded1c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(batch, logits):\n",
    "    gold = format_for_output(batch['raws'])\n",
    "    hypotheses: List[Dict] = []\n",
    "    logits = logits.transpose(1, 2)\n",
    "    assert logits.size(1) == len(LABELS), \"expects the label in dim=1\"\n",
    "    \n",
    "    for i in range(logits.size(0)):\n",
    "        hypothesis = parse_label_encoding(None, batch['encodings'][i], logits[i], LABELS)\n",
    "        hypotheses.append({\n",
    "            \"id\": batch['raws'][i]['id'],\n",
    "            \"labels\": hypothesis\n",
    "        })\n",
    "        \n",
    "    hypotheses = format_for_output(hypotheses)\n",
    "    \n",
    "    res_for_screen, f1, f1_per_label = t1scorer.FLC_score_to_string(gold, hypotheses, per_label=True)\n",
    "    \n",
    "    metrics = {\n",
    "        'f1': f1,\n",
    "        **f1_per_label\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05777e90-adc2-4980-ab42-8a921590e139",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m3\u001b[39m, MAX_LENGTH, \u001b[38;5;28mlen\u001b[39m(LABELS)))\n\u001b[1;32m      3\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m>\u001b[39m THRESHOLD\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(batch, logits)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(batch, logits):\n\u001b[0;32m----> 2\u001b[0m     gold \u001b[38;5;241m=\u001b[39m \u001b[43mformat_for_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraws\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     hypotheses: List[Dict] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mformat_for_output\u001b[0;34m(objs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_for_output\u001b[39m(objs):\n\u001b[0;32m----> 2\u001b[0m     labels_per_par \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objs:\n\u001b[1;32m      4\u001b[0m         par_id \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0\n",
    "logits = torch.randn((3, MAX_LENGTH, len(LABELS)))\n",
    "logits = logits > THRESHOLD\n",
    "compute_metrics(batch, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a893f96-70c4-43cb-9603-ac1e5bf47f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['tensors'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b2f6ea-2eda-4f61-b4ca-fe07f3da3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(message)s\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_dir):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    path = Path(checkpoint_dir) / \"model_best.pt\"\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict()\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "\n",
    "\n",
    "def validation(model, valid_dl, max_step):\n",
    "    model.eval()\n",
    "    loss_across_batches = []\n",
    "    metric_across_batches = dict(zip(['f1', ] + LABELS, [0.0, ] * 24))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(valid_dl, leave=False)\n",
    "        for step_no, batch in enumerate(bar):\n",
    "            for key in batch['tensors'].keys():\n",
    "                batch['tensors'][key] = batch['tensors'][key].to(device)\n",
    "            \n",
    "            # calculate loss on valid\n",
    "            loss, logits = step(model, batch['tensors'])\n",
    "            loss_across_batches.append(loss.item())\n",
    "\n",
    "            metrics = compute_metrics(batch, logits)\n",
    "            # sum up\n",
    "            for k, v in metrics.items():\n",
    "                metric_across_batches[k] += v\n",
    "\n",
    "            if step_no == max_step:\n",
    "                break\n",
    "\n",
    "        bar.close()\n",
    "\n",
    "        # we need the mean\n",
    "        for k,v in metric_across_batches.items():\n",
    "            metric_across_batches[k] /= (step_no + 1)\n",
    "\n",
    "    return {\"loss\": mean(loss_across_batches), **metric_across_batches}\n",
    "\n",
    "\n",
    "def step(model, batch):\n",
    "    output: TokenClassifierOutput = model(input_ids=batch['input_ids'],\n",
    "                                          token_type_ids=batch['token_type_ids'],\n",
    "                                          attention_mask=batch['attention_mask'],\n",
    "                                          labels=batch['labels'] if 'labels' in batch else None)\n",
    "    \n",
    "    return output['loss'] if 'loss' in output else None, output['logits']\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    optimizer,\n",
    "    train_dl: torch.utils.data.DataLoader,\n",
    "    valid_dl: torch.utils.data.DataLoader,\n",
    "    config: dict,\n",
    "    args,\n",
    "    lr_scheduler=None,\n",
    "    checkpoint_dir=\"./checkpoint\",\n",
    "    max_step=-1,\n",
    "    experiment_name=None,\n",
    "    epoch=0,\n",
    "):\n",
    "\n",
    "    best_f1 = float(\"-inf\")\n",
    "\n",
    "    _training_history = {\n",
    "        \"train/loss\": [],\n",
    "        \"valid/loss\": [],\n",
    "        \"valid/f1\": []\n",
    "    }\n",
    "    _validation_history = {f\"valid/{k}\" :[] for k in LABELS}\n",
    "    history = {**_training_history, **_validation_history}\n",
    "\n",
    "    for epoch in range(epoch + 1, epoch + config[\"max_epoch\"] + 1):\n",
    "        model.train()\n",
    "        loss_across_batches = []\n",
    "        bar = tqdm(train_dl, unit=\"batch\")\n",
    "\n",
    "        for step_no, batch in enumerate(bar):\n",
    "            # move to gpu\n",
    "            for key in batch['tensors'].keys():\n",
    "                batch['tensors'][key] = batch['tensors'][key].to(device)\n",
    "        \n",
    "            # reset grads\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step forward\n",
    "            loss, logits = step(model, batch['tensors'])\n",
    "\n",
    "            # step backward\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_across_batches.append(loss.item())\n",
    "\n",
    "            # show each batch loss in tqdm bar\n",
    "            bar.set_postfix(**{\"loss\": loss.item()})\n",
    "        \n",
    "            # skip training on the entire training dataset\n",
    "            # useful during debugging\n",
    "            if step_no == max_step:\n",
    "                break\n",
    "\n",
    "        validation_metrics = validation(model, valid_dl, max_step)\n",
    "\n",
    "        history[\"train/loss\"].append(mean(loss_across_batches))\n",
    "        for k, v in validation_metrics.items():\n",
    "            history[f\"valid/{k}\"].append(v)\n",
    "        \n",
    "\n",
    "        if validation_metrics[\"f1\"] > best_f1:\n",
    "            best_f1 = validation_metrics[\"f1\"]\n",
    "            save_checkpoint(model, optimizer, epoch, checkpoint_dir)\n",
    "            print(\"üéâ best f1 reached, saved a checkpoint.\")\n",
    "\n",
    "        log(epoch, history)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "def log(epoch, history):\n",
    "    print(\n",
    "        f\"Epoch: {epoch},\\tTrain Loss: {history['train/loss'][-1]},\\tVal Loss: {history['valid/loss'][-1]}\\tVal F1: {history['valid/f1'][-1]}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path, optimizer=None, lr_scheduler=None):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler_state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        logger.info(f\"üéâ Loaded existing model. Epoch: {checkpoint['epoch']}\")\n",
    "        return model, optimizer, lr_scheduler, epoch\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"No checkpoint found in the provided path\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed770b0-82ca-48d8-bcdd-312496128fb8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ad32e73-1531-4e06-bcf1-1a084459253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    # For reproducibility\n",
    "    return CustomBertForTokenClassification.from_pretrained(MODEL_NAME, output_hidden_states=True, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22949d54-4788-4d9d-981a-2bdda6c086c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of CustomBertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                  | 2/2333 [00:09<3:09:34,  4.88s/batch, loss=0.107]\n",
      "                                                                                                                                         \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 6\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 132\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, optimizer, train_dl, valid_dl, config, args, lr_scheduler, checkpoint_dir, max_step, experiment_name, epoch)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_no \u001b[38;5;241m==\u001b[39m max_step:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m validation_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(mean(loss_across_batches))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m validation_metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[16], line 51\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(model, valid_dl, max_step)\u001b[0m\n\u001b[1;32m     48\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m step(model, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensors\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     49\u001b[0m loss_across_batches\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 51\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# sum up\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(batch, logits)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(batch, logits):\n\u001b[0;32m----> 2\u001b[0m     gold \u001b[38;5;241m=\u001b[39m \u001b[43mformat_for_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraws\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     hypotheses: List[Dict] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mformat_for_output\u001b[0;34m(objs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_for_output\u001b[39m(objs):\n\u001b[0;32m----> 2\u001b[0m     labels_per_par \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objs:\n\u001b[1;32m      4\u001b[0m         par_id \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "model = model_init()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "config = {\n",
    "    'max_epoch': 2,\n",
    "}\n",
    "fit(model_init(), optimizer, train_dl, val_dl, config, None, max_step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c533de-7ed0-4119-b1fb-bd13a3c2d36a",
   "metadata": {},
   "source": [
    "## No Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532047c1-da91-4456-9850-0a624ad73948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\n",
    "            MODEL_NAME, num_labels=23, trust_remote_code=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58b4c8-2e01-47e5-827f-3de708af4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomBertForTokenClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233411bd-de34-4f74-8f02-34dd52d143d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, text):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # calculate loss on valid\n",
    "        _, logits = step(model, encoding)\n",
    "        logits = logits[0].transpose(0,1)\n",
    "        \n",
    "        # returns a list of span\n",
    "        hypothesis = parse_label_encoding(text, encoding, logits, LABELS)\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a00e97-91b6-4dbb-ab24-53382e69b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, tokenizer, \"ÿ™ÿ≠ÿ∞Ÿäÿ±ÿßÿ™ ŸÖŸÜ ÿ≠ÿ±ÿ® ÿ¨ÿØŸäÿØÿ© ŸÅŸä ÿ≠ÿßŸÑ ŸÅÿ¥ŸÑ ÿßŸÑÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™ ÿßŸÑŸÇÿßÿØŸÖÿ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357277d-ee6b-4c33-970e-ec7bcbe812b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(filepath, model, tokenizer):\n",
    "    \"\"\"\n",
    "    takes a filepath and saves output following the shared task's format and metrics if labels are available\n",
    "    \"\"\"\n",
    "    infile = jsonlines.open(filepath)\n",
    "    outpath = filepath + \".hyp\"\n",
    "    outfile = open(outpath, 'w', encoding=\"utf-8\")\n",
    "\n",
    "    for sample in tqdm(infile):\n",
    "        hypothesis = generate(model, tokenizer, sample['text'])\n",
    "        outfile.write(json.dumps({'id': sample['id'], 'labels': hypothesis}, ensure_ascii=False) + \"\\n\")\n",
    "    infile.close()\n",
    "    outfile.close()\n",
    "    \n",
    "    print('üéâ output saved to', outpath)\n",
    "\n",
    "\n",
    "testfile = \"../data/dummytest.jsonl\"\n",
    "evaluate(testfile, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d46a45-2bdc-43d3-a53a-f2de16f00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4d7ff-6cc4-4bb2-a220-1c26149d52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train/loss': [0.6891509493192037, 0.6939094463984171],\n",
    "    'valid/loss': [0.697946031888326, 0.697946031888326],\n",
    "    'valid/f1': [0.04855302080998519, 0.04855302080998519],\n",
    "    'valid/Appeal_to_Values': [0.0, 0.0],\n",
    "    'valid/Loaded_Language': [0.08980785296574771, 0.08980785296574771],\n",
    "    'valid/Consequential_Oversimplification': [0.0, 0.0],\n",
    "    'valid/Causal_Oversimplification': [0.0, 0.0],\n",
    "    'valid/Questioning_the_Reputation': [0.3707482993197278, 0.3707482993197278],\n",
    "    'valid/Straw_Man': [0.0, 0.0],\n",
    "    'valid/Repetition': [0.0, 0.0],\n",
    "    'valid/Guilt_by_Association': [0.0, 0.0],\n",
    "    'valid/Appeal_to_Hypocrisy': [0.0, 0.0],\n",
    "    'valid/Conversation_Killer': [0.0, 0.0],\n",
    "    'valid/False_Dilemma-No_Choice': [0.30303030303030304, 0.30303030303030304],\n",
    "    'valid/Whataboutism': [0.0, 0.0],\n",
    "    'valid/Slogans': [0.0, 0.0],\n",
    "    'valid/Obfuscation-Vagueness-Confusion': [0.05128205128205129, 0.05128205128205129],\n",
    "    'valid/Name_Calling-Labeling': [0.14531590413943354, 0.14531590413943354],\n",
    "    'valid/Flag_Waving': [0.0, 0.0],\n",
    "    'valid/Doubt': [0.0, 0.0],\n",
    "    'valid/Appeal_to_Fear-Prejudice': [0.0, 0.0],\n",
    "    'valid/Exaggeration-Minimisation': [0.0, 0.0],\n",
    "    'valid/Red_Herring': [0.0, 0.0],\n",
    "    'valid/Appeal_to_Popularity': [0.0, 0.0],\n",
    "    'valid/Appeal_to_Authority': [0.1568627450980392, 0.1568627450980392],\n",
    "    'valid/Appeal_to_Time': [0.0, 0.0]\n",
    "}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,5), dpi=300)\n",
    "ax.plot(history['train/loss'], linestyle='-', label='train')\n",
    "ax.plot(history['valid/loss'], linestyle='--', label='valid')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43191f2d-2ade-4e34-8e01-7a980bc9b408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2657e1-e8c2-4904-a73d-a90212510c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
