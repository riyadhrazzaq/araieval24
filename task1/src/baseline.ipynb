{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947844d3-0f10-409b-ad06-3899f9322960",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "051f1f0a-3d53-4f44-959b-07623f2966fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c27cbcc-c16c-4eaf-a157-3e7924dbad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "import datautil as dutil\n",
    "from datautil import tokenizer\n",
    "import evalutil as eutil\n",
    "from model import *\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ffa1d8-6583-41e3-bbab-754164c94965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on /home/riyadh/codes/nlp/araieval_arabicnlp24/task1/src/../data/araieval24_task1_train.jsonl\n",
      "Validating on /home/riyadh/codes/nlp/araieval_arabicnlp24/task1/src/../data/araieval24_task1_dev.jsonl\n",
      "\n",
      "{'id': '7365', 'text': 'تحذيرات من حرب جديدة في حال فشل الانتخابات القادمة', 'labels': [{'start': 0, 'end': 50, 'technique': 'Appeal_to_Fear-Prejudice', 'text': 'تحذيرات من حرب جديدة في حال فشل الانتخابات القادمة'}, {'start': 11, 'end': 14, 'technique': 'Loaded_Language', 'text': 'حرب'}], 'type': 'tweet'}\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'encoding'])\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path(config.root)\n",
    "TRAIN_FILE = ROOT / \"araieval24_task1_train.jsonl\"\n",
    "DEV_FILE = ROOT / \"araieval24_task1_dev.jsonl\"\n",
    "\n",
    "print(f\"Training on {TRAIN_FILE.absolute()}\\nValidating on {DEV_FILE.absolute()}\\n\")\n",
    "\n",
    "with jsonlines.open(TRAIN_FILE) as jsonfile:\n",
    "    for obj in jsonfile:\n",
    "        print(obj)\n",
    "\n",
    "        parsed = dutil.parse_sample(obj)\n",
    "        print(parsed.keys())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165825a-201b-47e4-87f2-10ea5618899b",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0440705c-8359-49a1-a2ec-86633753aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_json(str(TRAIN_FILE))\n",
    "val_ds = Dataset.from_json(str(DEV_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e44654e0-aa7e-461f-832a-a79f328d26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.select(range(0, 10))\n",
    "val_ds = val_ds.select(range(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "932c5005-3f7a-4afc-ab03-d827629c329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_ds = train_ds.map(\n",
    "    dutil.parse_sample, remove_columns=train_ds.column_names\n",
    ")\n",
    "\n",
    "tokenized_val_ds = val_ds.map(dutil.parse_sample, remove_columns=val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "00f57e47-075b-46ca-b804-ea271ebc382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "11f89c64-4144-4217-a767-9acae916b6cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mtokenized_train_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoding'"
     ]
    }
   ],
   "source": [
    "type(tokenized_train_ds[0][\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd967008-884c-411f-bfb8-6b11bd610428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1141390-1436-42d4-92bd-44cfc1048590",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58638463-65cb-475f-bab0-2585af88949e",
   "metadata": {},
   "source": [
    "# Evaluation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55048b98-e045-428d-87d1-e73ce7995c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "n_labels = len(id2label)\n",
    "\n",
    "\n",
    "def divide(a: int, b: int):\n",
    "    return a / b if b > 0 else 0\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Customize the `compute_metrics` of `transformers`\n",
    "    Args:\n",
    "        - p (tuple):      2 numpy arrays: predictions and true_labels\n",
    "    Returns:\n",
    "        - metrics (dict): f1 score on\n",
    "    \"\"\"\n",
    "    # (1)\n",
    "    predictions, true_labels = p\n",
    "    # print(predictions.shape, true_labels.shape, type(predictions))\n",
    "\n",
    "    # (2)\n",
    "    predicted_labels = np.where(\n",
    "        predictions > 0, np.ones(predictions.shape), np.zeros(predictions.shape)\n",
    "    )\n",
    "    metrics = {}\n",
    "\n",
    "    # (3)\n",
    "    cm = multilabel_confusion_matrix(\n",
    "        true_labels.reshape(-1, n_labels), predicted_labels.reshape(-1, n_labels)\n",
    "    )\n",
    "\n",
    "    # (4)\n",
    "    for label_idx, matrix in enumerate(cm):\n",
    "        if label_idx == 0:\n",
    "            continue  # We don't care about the label \"O\"\n",
    "        tp, fp, fn = matrix[1, 1], matrix[0, 1], matrix[1, 0]\n",
    "        precision = divide(tp, tp + fp)\n",
    "        recall = divide(tp, tp + fn)\n",
    "        f1 = divide(2 * precision * recall, precision + recall)\n",
    "        metrics[f\"f1_{id2label[label_idx]}\"] = f1\n",
    "\n",
    "    # (5)\n",
    "    macro_f1 = sum(list(metrics.values())) / (n_labels - 1)\n",
    "    metrics[\"macro_f1\"] = macro_f1\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# def compute_metric(data):\n",
    "#     hypotheses, reference = data\n",
    "#     hypotheses = np.where(\n",
    "#         hypotheses > 0, np.ones(hypotheses.shape), np.zeros(hypotheses.shape)\n",
    "#     )\n",
    "#     parse_label_encoding()\n",
    "\n",
    "#     metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f50bbc-3a58-4ba4-9cc0-a441c4ccb60a",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e3407-954f-44c7-9c5e-d735e61062f5",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e38d245-ee5a-4cbf-a39f-afae49a54e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_init\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# For reproducibility\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CustomBertForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(dutil\u001b[38;5;241m.\u001b[39mMODEL_NAME, id2label\u001b[38;5;241m=\u001b[39mid2label, label2id\u001b[38;5;241m=\u001b[39mlabel2id)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[0;32m----> 7\u001b[0m     args\u001b[38;5;241m=\u001b[39m\u001b[43mtraining_args\u001b[49m,\n\u001b[1;32m      8\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_ds,\n\u001b[1;32m      9\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_val_ds,\n\u001b[1;32m     10\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     11\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     12\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_args' is not defined"
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    # For reproducibility\n",
    "    return CustomBertForTokenClassification.from_pretrained(dutil.MODEL_NAME, id2label=id2label, label2id=label2id)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ba34e-2f91-4f93-aa5f-8bf25a855cbd",
   "metadata": {},
   "source": [
    "## Todo\n",
    "1. Write a method to transform tensor labels to tags in evaluation step.\n",
    "2.    write a collate fn that also returns a list of encoding object, instead of just the batchencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f2643bea-fb03-475f-b64d-057f7df91275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'dict'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CharSpan(start=8, end=12)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = {\n",
    "    \"id\": \"AFP_458-eurl_02_004\",\n",
    "    \"text\": \"كان بطل فقرة بروباغندا في الحلقة الأولى هو صلاح قوش ، المعادل الموضوعي لعُمر سليمان الرئيس الأسبق للمخابرات المصرية. وكما عمر سليمان، نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة.\",\n",
    "    \"labels\": [\n",
    "        {\"technique\": \"Name_Calling-Labeling\", \"text\": \"بطل\", \"start\": 4, \"end\": 7},\n",
    "        {\n",
    "            \"technique\": \"Obfuscation-Vagueness-Confusion\",\n",
    "            \"text\": \"نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة\",\n",
    "            \"start\": 134,\n",
    "            \"end\": 182,\n",
    "        },\n",
    "        {\n",
    "            \"technique\": \"Loaded_Language\",\n",
    "            \"text\": \"الأساطير الغامضة\",\n",
    "            \"start\": 166,\n",
    "            \"end\": 182,\n",
    "        },\n",
    "    ],\n",
    "    \"type\": \"paragraph\",\n",
    "}\n",
    "\n",
    "obj2 = dutil.parse_sample(train_ds[0])\n",
    "dutil.parse_sample(obj)[\"encoding\"].word_to_chars(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0ad33e0-e141-423a-ae8d-a5be7f99f540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharSpan(start=8, end=12)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj2[\"encoding\"].word_to_chars(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f84d2f-42cd-4d3b-acab-8ee57f7eac42",
   "metadata": {},
   "source": [
    "## Utility for Datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65441e6e-339c-4ec3-ab2d-0cb7ab8bb147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_ds[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85084bdc-7a4d-4d4d-b612-eae2d3a4b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, data):\n",
    "        data = collate_fn(data)\n",
    "        fast_encodings = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "649211df-36a9-4b33-bb2b-24c38e334427",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = CollateFn(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c02adde-3f50-4495-b0c6-321aeb8a71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(tokenized_train_ds, batch_size=3, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b049d75a-fa17-44da-9fc6-016df9ff93f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n"
     ]
    }
   ],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3db378-98bd-4319-ab7c-46e55fa3e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeni"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
