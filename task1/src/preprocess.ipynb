{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc7c947-1fc0-49a8-98ec-38475a36bcf2",
   "metadata": {},
   "source": [
    "# Variable Naming Convention\n",
    "unique labels: `LABELS`\n",
    "\n",
    "one-hot encoded labels for samples: `labels_tokens`\n",
    "\n",
    "dictionaries that gives ranges of labels either word-level or char-level: `*_spans`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9555c0-5522-4323-b288-c2ba69427bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install -q jupyter-black\n",
    "%load_ext jupyter_black\n",
    "# !python -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6ed39a-bdcb-4642-82b8-8fc5bd0db7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from statistics import mean\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.punctuation import TOKENIZER_INFIXES\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2060f3d8-8446-4184-bd4f-39303c64ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\"../data/\")\n",
    "\n",
    "LABELS = [\n",
    "    \"Appeal_to_Values\",\n",
    "    \"Loaded_Language\",\n",
    "    \"Consequential_Oversimplification\",\n",
    "    \"Causal_Oversimplification\",\n",
    "    \"Questioning_the_Reputation\",\n",
    "    \"Straw_Man\",\n",
    "    \"Repetition\",\n",
    "    \"Guilt_by_Association\",\n",
    "    \"Appeal_to_Hypocrisy\",\n",
    "    \"Conversation_Killer\",\n",
    "    \"False_Dilemma-No_Choice\",\n",
    "    \"Whataboutism\",\n",
    "    \"Slogans\",\n",
    "    \"Obfuscation-Vagueness-Confusion\",\n",
    "    \"Name_Calling-Labeling\",\n",
    "    \"Flag_Waving\",\n",
    "    \"Doubt\",\n",
    "    \"Appeal_to_Fear-Prejudice\",\n",
    "    \"Exaggeration-Minimisation\",\n",
    "    \"Red_Herring\",\n",
    "    \"Appeal_to_Popularity\",\n",
    "    \"Appeal_to_Authority\",\n",
    "    \"Appeal_to_Time\",\n",
    "]\n",
    "\n",
    "WHITESPACE_PLACEHOLDER = \"▁\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15de1b-aa0b-4599-bc82-4837cbc092b5",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c52b1476-fbc6-4f6c-9e6c-1cbb8d267e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokenizer.add_tokens(WHITESPACE_PLACEHOLDER, special_tokens=True)\n",
    "\n",
    "# spacy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "infixes = TOKENIZER_INFIXES + ([r\"[▁:,]\"])\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fcdc5-deed-49e1-9651-daf2cc830af0",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84282d5c-aceb-46c8-a90d-9eebdaa11e15",
   "metadata": {},
   "source": [
    "## Text Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f63ba9-7808-4ebb-a70b-c0c9482c3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text, span_objs, tokenizer, labels, whitespace_placeholder, nlp):\n",
    "    missed = 0\n",
    "    text = text.lower().replace(\" \", whitespace_placeholder)\n",
    "    doc = nlp(text)[:]  # slicing to convert into an Span object\n",
    "    # creates a dict of idx to labels. idx is the index of the word in the sentence.\n",
    "    labels_per_word = {i: list() for i in range(len(doc))}\n",
    "\n",
    "    spans = [(span[\"start\"], span[\"end\"], span[\"technique\"]) for span in span_objs]\n",
    "\n",
    "    for start, end, label in spans:\n",
    "        span = doc.char_span(start, end)\n",
    "        # sometimes span will be None when start and end does not give corresponds to valid span\n",
    "        # mostly because of unsanitized texts\n",
    "        if span:\n",
    "            # span.start, span.end are word level spans\n",
    "            for i in range(span.start, span.end):\n",
    "                labels_per_word[i].append(label)\n",
    "        else:\n",
    "            missed += 1\n",
    "\n",
    "    tokens = []\n",
    "    labels_per_token = [list() for _ in labels]\n",
    "\n",
    "    for i, word in enumerate(doc):\n",
    "        tokens_per_word = tokenizer.tokenize(word.text)\n",
    "        tokens.extend(tokens_per_word)\n",
    "\n",
    "        for j, label in enumerate(labels):\n",
    "            if label in labels_per_word[i]:\n",
    "                # if current label is assigned to this word, duplicate this label required amount of times\n",
    "                labels_per_token[j].extend(\n",
    "                    [\n",
    "                        1,\n",
    "                    ]\n",
    "                    * len(tokens_per_word)\n",
    "                )\n",
    "            else:\n",
    "                labels_per_token[j].extend(\n",
    "                    [\n",
    "                        0,\n",
    "                    ]\n",
    "                    * len(tokens_per_word)\n",
    "                )\n",
    "\n",
    "    return tokens, labels_per_token, labels_per_word, missed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8ddd8-33df-4a8b-82f1-614aa81520eb",
   "metadata": {},
   "source": [
    "## File Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b07939-729f-4127-b64d-4999d456ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_jsonl(path, labels, tokenizer):\n",
    "    total = 0\n",
    "    missed = 0\n",
    "\n",
    "    with jsonlines.open(path) as f:\n",
    "        batch_tokens = []  # (N, L)\n",
    "        batch_labels = []  # (N, C, L)\n",
    "\n",
    "        for obj in tqdm(f):\n",
    "            doc_id = str(obj[\"id\"])\n",
    "            text = obj[\"text\"]\n",
    "            total += len(obj[\"labels\"])\n",
    "            try:\n",
    "                tokens, label, _, _missed = parse_text(\n",
    "                    text, obj[\"labels\"], tokenizer, labels, WHITESPACE_PLACEHOLDER, nlp\n",
    "                )\n",
    "\n",
    "                batch_tokens.append(tokens)\n",
    "                batch_labels.append(label)\n",
    "\n",
    "                missed += _missed\n",
    "            except Exception as e:\n",
    "                # print(doc_id, text, label, sep=\"\\n\")\n",
    "                raise e\n",
    "\n",
    "    return batch_tokens, batch_labels, missed, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f230d663-82a3-4354-8862-4267c392c478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def agg_sum(array):\n",
    "    result = []\n",
    "    running_sum = 0\n",
    "    for data in array:\n",
    "        running_sum += data\n",
    "        result.append(running_sum)\n",
    "    return result\n",
    "\n",
    "\n",
    "agg_sum([1, 2, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b09fa98-0a8b-4df3-9090-6e3887c04a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (4, 6)]\n"
     ]
    }
   ],
   "source": [
    "def find_consecutive_trues(flags):\n",
    "    \"\"\"\n",
    "    This function takes an array of boolean flags and returns a list of ranges\n",
    "    of all consecutive true values.\n",
    "\n",
    "    Args:\n",
    "        flags: A list of boolean flags.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple represents a range of consecutive\n",
    "        true values. The tuple contains the starting and ending indices (inclusive)\n",
    "        of the range.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    start_idx = None\n",
    "    for i, flag in enumerate(flags):\n",
    "        if flag and start_idx is None:\n",
    "            start_idx = i\n",
    "        elif not flag and start_idx is not None:\n",
    "            ranges.append((start_idx, i - 1))\n",
    "            start_idx = None\n",
    "    if start_idx is not None:\n",
    "        ranges.append((start_idx, len(flags) - 1))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "# Example usage\n",
    "flags = [1, 1, 0, 0, 1, 1, 1]\n",
    "ranges = find_consecutive_trues(flags)\n",
    "print(ranges)  # Output: [(0, 1), (3, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14944a7e-47e6-4bb6-8bbf-c2e0ff119b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_span_to_char_span(tokens, labels_tokens, labels, whitespace_placeholder):\n",
    "    \"\"\"\n",
    "    transform word level tags spans to char spans\n",
    "\n",
    "    Args:\n",
    "        text: list of token, (N, L)\n",
    "        spans: list of list of labels (N, C, L)\n",
    "        labels: list of available tags (C,)\n",
    "    \"\"\"\n",
    "    # [{\"technique\": \"Exaggeration-Minimisation\", \"start\": 13, \"end\": 32, \"text\": \"ن السعوديه تاوي اره\"}\n",
    "    span_objs = []\n",
    "\n",
    "    decoded_text = (\n",
    "        \"\".join(tokens).replace(\"##\", \"\").replace(whitespace_placeholder, \" \")\n",
    "    )\n",
    "\n",
    "    lengths = [len(token.replace(\"##\", \"\")) for token in tokens]\n",
    "    char_boundaries = agg_sum(lengths)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        span_ranges = find_consecutive_trues(labels_tokens[i])\n",
    "        for start_idx, stop_idx in span_ranges:\n",
    "            start_idx -= 1\n",
    "            if start_idx < 0:\n",
    "                char_start_idx = 0\n",
    "            else:\n",
    "                char_start_idx = char_boundaries[start_idx]\n",
    "\n",
    "            char_stop_idx = char_boundaries[stop_idx]\n",
    "\n",
    "            span_text = decoded_text[char_start_idx:char_stop_idx]\n",
    "\n",
    "            obj = {\n",
    "                \"technique\": label,\n",
    "                \"start\": char_start_idx,\n",
    "                \"end\": char_stop_idx,\n",
    "                \"text\": span_text,\n",
    "            }\n",
    "\n",
    "            span_objs.append(obj)\n",
    "\n",
    "    return decoded_text, span_objsdef token_span_to_char_span(tokens, labels_tokens, labels, whitespace_placeholder):\n",
    "    \"\"\"\n",
    "    transform word level tags spans to char spans\n",
    "\n",
    "    Args:\n",
    "        text: list of token, (N, L)\n",
    "        spans: list of list of labels (N, C, L)\n",
    "        labels: list of available tags (C,)\n",
    "    \"\"\"\n",
    "    # [{\"technique\": \"Exaggeration-Minimisation\", \"start\": 13, \"end\": 32, \"text\": \"ن السعوديه تاوي اره\"}\n",
    "    span_objs = []\n",
    "\n",
    "    decoded_text = (\n",
    "        \"\".join(tokens).replace(\"##\", \"\").replace(whitespace_placeholder, \" \")\n",
    "    )\n",
    "\n",
    "    lengths = [len(token.replace(\"##\", \"\")) for token in tokens]\n",
    "    char_boundaries = agg_sum(lengths)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        span_ranges = find_consecutive_trues(labels_tokens[i])\n",
    "        for start_idx, stop_idx in span_ranges:\n",
    "            start_idx -= 1\n",
    "            if start_idx < 0:\n",
    "                char_start_idx = 0\n",
    "            else:\n",
    "                char_start_idx = char_boundaries[start_idx]\n",
    "\n",
    "            char_stop_idx = char_boundaries[stop_idx]\n",
    "\n",
    "            span_text = decoded_text[char_start_idx:char_stop_idx]\n",
    "\n",
    "            obj = {\n",
    "                \"technique\": label,\n",
    "                \"start\": char_start_idx,\n",
    "                \"end\": char_stop_idx,\n",
    "                \"text\": span_text,\n",
    "            }\n",
    "\n",
    "            span_objs.append(obj)\n",
    "\n",
    "    return decoded_text, span_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3a8cd-2a80-4bf8-91e6-3c92ed71eb1b",
   "metadata": {},
   "source": [
    "## Test The Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8108d10d-360f-4e44-8fb7-f3aa1ea83c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = {\n",
    "    \"id\": \"AFP_458-eurl_02_004\",\n",
    "    \"text\": \"كان بطل فقرة بروباغندا في الحلقة الأولى هو صلاح قوش ، المعادل الموضوعي لعُمر سليمان الرئيس الأسبق للمخابرات المصرية. وكما عمر سليمان، نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة.\",\n",
    "    \"labels\": [\n",
    "        {\"technique\": \"Name_Calling-Labeling\", \"text\": \"بطل\", \"start\": 4, \"end\": 7},\n",
    "        {\n",
    "            \"technique\": \"Obfuscation-Vagueness-Confusion\",\n",
    "            \"text\": \"نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة\",\n",
    "            \"start\": 134,\n",
    "            \"end\": 182,\n",
    "        },\n",
    "        {\n",
    "            \"technique\": \"Loaded_Language\",\n",
    "            \"text\": \"الأساطير الغامضة\",\n",
    "            \"start\": 166,\n",
    "            \"end\": 182,\n",
    "        },\n",
    "    ],\n",
    "    \"type\": \"paragraph\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe40b23-c956-44b3-9918-013363ecaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = {\n",
    "#     \"text\": \"Newt Gingrich: The truth about Trump, Putin, and Obama\\n\\nPresident Trump.\",\n",
    "#     \"labels\": [\n",
    "#         {\n",
    "#             \"technique\": \"Name_Calling-Labeling\",\n",
    "#             \"text\": \"Gingrich:\",\n",
    "#             \"start\": 5,\n",
    "#             \"end\": 14,\n",
    "#         },\n",
    "#         {\n",
    "#             \"technique\": \"Obfuscation-Vagueness-Confusion\",\n",
    "#             \"text\": \"The truth about Trump\",\n",
    "#             \"start\": 15,\n",
    "#             \"end\": 36,\n",
    "#         },\n",
    "#         {\n",
    "#             \"technique\": \"Loaded_Language\",\n",
    "#             \"text\": \"الأساطير الغامضة\",\n",
    "#             \"start\": 166,\n",
    "#             \"end\": 182,\n",
    "#         },\n",
    "#     ],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7541bbb5-eafc-4252-a8b9-60db0032c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, labels_tokens, labels_per_token, _ = parse_text(\n",
    "    obj[\"text\"],\n",
    "    obj[\"labels\"],\n",
    "    tokenizer,\n",
    "    LABELS,\n",
    "    WHITESPACE_PLACEHOLDER,\n",
    "    nlp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8698ce-bb7e-4c40-992c-c4b3a8201747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens/Word Pieces\n",
      "['كان', '▁', 'ب', '##طل', '▁', 'ف', '##قر', '##ة', '▁', 'بر', '##وب', '##اغ', '##ندا', '▁', 'في', '▁', 'ال', '##حل', '##قة', '▁', 'الأولى', '▁', 'هو', '▁', 'صلاح', '▁', 'ق', '##وش', '▁', '،', '▁', 'ال', '##مع', '##ادل', '▁', 'ال', '##مو', '##ض', '##وعي', '▁', 'ل', '##ع', '##ُ', '##مر', '▁', 'سليمان', '▁', 'الرئيس', '▁', 'ال', '##أس', '##بق', '▁', 'ل', '##لم', '##خ', '##اب', '##رات', '▁', 'المصرية', '.', '▁', 'و', '##كم', '##ا', '▁', 'عمر', '▁', 'سليمان', '،', '▁', 'ن', '##ُ', '##س', '##ج', '##ت', '▁', 'حول', '▁', 'ق', '##وش', '▁', 'وأن', '##ش', '##ط', '##ته', '▁', 'العديد', '▁', 'من', '▁', 'ال', '##أس', '##اطي', '##ر', '▁', 'ال', '##غا', '##م', '##ضة', '.']\n",
      "23 101\n",
      "\n",
      "Labels assigned to each tokens\n",
      "{0: [], 1: [], 2: ['Name_Calling-Labeling'], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: [], 22: [], 23: [], 24: [], 25: [], 26: [], 27: [], 28: [], 29: [], 30: [], 31: [], 32: [], 33: [], 34: [], 35: [], 36: [], 37: [], 38: [], 39: [], 40: [], 41: [], 42: [], 43: [], 44: ['Obfuscation-Vagueness-Confusion'], 45: ['Obfuscation-Vagueness-Confusion'], 46: ['Obfuscation-Vagueness-Confusion'], 47: ['Obfuscation-Vagueness-Confusion'], 48: ['Obfuscation-Vagueness-Confusion'], 49: ['Obfuscation-Vagueness-Confusion'], 50: ['Obfuscation-Vagueness-Confusion'], 51: ['Obfuscation-Vagueness-Confusion'], 52: ['Obfuscation-Vagueness-Confusion'], 53: ['Obfuscation-Vagueness-Confusion'], 54: ['Obfuscation-Vagueness-Confusion'], 55: ['Obfuscation-Vagueness-Confusion'], 56: ['Obfuscation-Vagueness-Confusion', 'Loaded_Language'], 57: ['Obfuscation-Vagueness-Confusion', 'Loaded_Language'], 58: ['Obfuscation-Vagueness-Confusion', 'Loaded_Language'], 59: []}\n",
      "\n",
      "Original Char-level Spans\n",
      "[{'technique': 'Name_Calling-Labeling', 'text': 'بطل', 'start': 4, 'end': 7}, {'technique': 'Obfuscation-Vagueness-Confusion', 'text': 'نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة', 'start': 134, 'end': 182}, {'technique': 'Loaded_Language', 'text': 'الأساطير الغامضة', 'start': 166, 'end': 182}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens/Word Pieces\")\n",
    "print(tokens)\n",
    "\n",
    "print(len(labels_tokens), len(labels_tokens[0]))\n",
    "print()\n",
    "\n",
    "print(\"Labels assigned to each tokens\")\n",
    "print(labels_per_token)\n",
    "\n",
    "print(\"\\nOriginal Char-level Spans\")\n",
    "print(obj[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fde468b-80ed-478e-a59a-51b9ffe62c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text, span_objs = token_span_to_char_span(\n",
    "    tokens, labels_tokens, LABELS, WHITESPACE_PLACEHOLDER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "278360fe-077a-44cb-8f06-6afa3d63aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "كان بطل فقرة بروباغندا في الحلقة الأولى هو صلاح قوش ، المعادل الموضوعي لعُمر سليمان الرئيس الأسبق للمخابرات المصرية. وكما عمر سليمان، نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة.\n",
      "كان بطل فقرة بروباغندا في الحلقة الأولى هو صلاح قوش ، المعادل الموضوعي لعُمر سليمان الرئيس الأسبق للمخابرات المصرية. وكما عمر سليمان، نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة.\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text, obj[\"text\"], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f63b3e2-e237-46a9-9cf7-0ea87e79a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'technique': 'Loaded_Language',\n",
       "  'start': 166,\n",
       "  'end': 182,\n",
       "  'text': 'الأساطير الغامضة'},\n",
       " {'technique': 'Obfuscation-Vagueness-Confusion',\n",
       "  'start': 134,\n",
       "  'end': 182,\n",
       "  'text': 'نُسجت حول قوش وأنشطته العديد من الأساطير الغامضة'},\n",
       " {'technique': 'Name_Calling-Labeling', 'start': 4, 'end': 7, 'text': 'بطل'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc03df-1abb-40ad-907d-659fa134a18f",
   "metadata": {},
   "source": [
    "# Load The Training Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1648aa8a-cb5f-4ee1-b1e1-51ec3c5b7cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['license_cc_by-nc-sa_4.0.txt',\n",
       " 'araieval24_task1_dev.jsonl',\n",
       " 'araieval24_task1_train.jsonl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8629c815-8308-46a0-9c2f-7a66c626f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [00:56, 124.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels, missed, total = parse_jsonl(\n",
    "    ROOT / \"araieval24_task1_train.jsonl\", LABELS, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99b4d051-fc36-4cb1-a567-d028384df3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6997 6997 3914 15765\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tokens), len(train_labels), missed, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "002c9713-3f90-443d-b25a-c26da43a3019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "921it [00:08, 113.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921 921 497 2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_tokens, dev_labels, dev_missed, dev_total = parse_jsonl(\n",
    "    ROOT / \"araieval24_task1_dev.jsonl\", LABELS, tokenizer\n",
    ")\n",
    "\n",
    "print(len(dev_tokens), len(dev_labels), dev_missed, dev_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328df6fd-5d26-4936-ad46-5556e47abfa3",
   "metadata": {},
   "source": [
    "# BertTokenizer w/ Offset Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43939b1f-d07b-40db-ac90-81b21b1dbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "fast_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5267d9e1-2c64-480d-acfc-16c4a67a70d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 177, 69110, 119, 29956, 24176, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (2, 5), (5, 6), (7, 9), (9, 12), (12, 13), (0, 0)]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = fast_tokenizer(\"i eat. risce!\", return_offsets_mapping=True)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbf5c943-00f9-4643-adb7-cdd8d8679ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'eat', '.', 'ri', '##sce', '!', '[SEP]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "701b0de8-208c-45b0-afb8-60614efe66e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 4, None]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a13b3b7-7bb5-43d2-a0c5-159f1bb23061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (2, 5), (5, 6), (7, 9), (9, 12), (12, 13), (0, 0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c42b3-d548-4e53-ac6a-4e1458b1ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
